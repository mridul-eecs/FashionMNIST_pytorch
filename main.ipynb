{"cells":[{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":"True"},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":"import torch\nimport torchvision\nimport torchvision.transforms as transforms\ntorch.cuda.is_available()"},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":"%config IPCompleter.greedy=True"},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":"train_set= torchvision.datasets.FashionMNIST(\n    root= './data/FashionMNIST',\n    train= True,\n    download= True,\n    transform= transforms.Compose([\n        transforms.ToTensor()\n    ])\n)"},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":"import torch.nn as nn\n\nclass Network(nn.Module):\n    def __init__(self):\n        super(Network, self).__init__()\n        self.conv1= nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n        self.conv2= nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n        self.fc1= nn.Linear(in_features=12*4*4, out_features=120)\n        self.fc2= nn.Linear(in_features=120, out_features=60)\n        self.out= nn.Linear(in_features=60, out_features=10)\n\n    def forward(self, t):\n        # input layer\n        t=t\n        # hidden 1st convolutional layer\n        t=self.conv1(t)\n        t=nn.functional.relu(t)\n        t=nn.functional.max_pool2d(t, k/ernel_size=2, stride=2)\n        \n        # hidden 2nd convolutional layer\n        t=self.conv2(t)\n        t=nn.functional.relu(t)\n        t=nn.functional.max_pool2d(t, kernel_size=2, stride=2)        \n        \n        # flattening for linear layer\n        t=t.reshape(-1, 12*4*4)\n        \n        # linear layer\n        t=self.fc1(t)\n        t=nn.functional.relu(t)\n\n        t=self.fc2(t)\n        t=nn.functional.relu(t)\n\n        # output layer\n        t=self.out(t)\n        #t=F.softmax(t, dim=1)\n        \n        return t\n\n    # def __repr__(self):\n    #     return \"Convolutional Neural Network\""},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":"Network(\n  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=192, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=60, bias=True)\n  (out): Linear(in_features=60, out_features=10, bias=True)\n)"},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":"network= Network()\nnetwork"},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":"data_loader= torch.utils.data.DataLoader(\n    train_set,\n    batch_size=10\n)"},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"torch.Size([10, 1, 28, 28]) \n torch.Size([10])\n"}],"source":"batch= next(iter(data_loader))\nimages, labels= batch\nprint(images.shape, '\\n', labels.shape)"},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([[ 0.0296,  0.0420, -0.1422, -0.0768, -0.0829,  0.0502,  0.1075, -0.1292,\n         -0.0662,  0.0071],\n        [ 0.0327,  0.0314, -0.1511, -0.0702, -0.0703,  0.0627,  0.1116, -0.1262,\n         -0.0641, -0.0045],\n        [ 0.0073,  0.0436, -0.1291, -0.0961, -0.0809,  0.0655,  0.1244, -0.1453,\n         -0.0592, -0.0039],\n        [ 0.0141,  0.0397, -0.1386, -0.0893, -0.0783,  0.0628,  0.1185, -0.1387,\n         -0.0608, -0.0018],\n        [ 0.0246,  0.0407, -0.1355, -0.0664, -0.0695,  0.0575,  0.1256, -0.1312,\n         -0.0653, -0.0079],\n        [ 0.0293,  0.0394, -0.1461, -0.0802, -0.0821,  0.0527,  0.1107, -0.1292,\n         -0.0685,  0.0013],\n        [ 0.0162,  0.0440, -0.1503, -0.0969, -0.0889,  0.0407,  0.0994, -0.1284,\n         -0.0718,  0.0101],\n        [ 0.0369,  0.0341, -0.1468, -0.0730, -0.0763,  0.0493,  0.1061, -0.1252,\n         -0.0666,  0.0043],\n        [ 0.0016,  0.0449, -0.1373, -0.1037, -0.0827,  0.0691,  0.1131, -0.1468,\n         -0.0589, -0.0010],\n        [ 0.0193,  0.0369, -0.1457, -0.0962, -0.0818,  0.0616,  0.1106, -0.1355,\n         -0.0627,  0.0057]], grad_fn=<ThAddmmBackward>)\nlabels=  tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6])\n"}],"source":"preds= network(images)\nprint(preds)\nprint('labels= ', torch.argmax(preds, dim=1))"},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.uint8)"},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":"# comparision with the actual\ntorch.argmax(preds, dim=1).eq(labels)"},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":"tensor(0., dtype=torch.float64)"},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":"# accuracy percentage\ndef accuracy(preds, labels):\n    return ((torch.argmax(preds, dim=1).eq(labels).sum()).double()/ len(labels))\naccuracy(preds, labels)"},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":"tensor(0., dtype=torch.float64)"},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":"# accuracy percentage\ndef accuracy(preds, labels):\n    return ((torch.argmax(preds, dim=1).eq(labels).sum()).double()/ len(labels))\naccuracy(preds, labels)"},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":"2.3094775676727295"},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":"# loss function\nloss= nn.functional.cross_entropy(preds, labels)\nloss.item()"},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"None\ntensor([[[[-0.0022, -0.0020, -0.0016, -0.0017, -0.0023],\n          [-0.0026, -0.0027, -0.0038, -0.0039, -0.0037],\n          [-0.0028, -0.0026, -0.0034, -0.0023, -0.0035],\n          [-0.0024, -0.0029, -0.0034, -0.0035, -0.0019],\n          [-0.0029, -0.0030, -0.0041, -0.0030, -0.0027]]],\n\n\n        [[[ 0.0019,  0.0008,  0.0011,  0.0014,  0.0005],\n          [ 0.0022,  0.0016,  0.0011, -0.0000, -0.0016],\n          [ 0.0017,  0.0007,  0.0000, -0.0009, -0.0013],\n          [ 0.0010,  0.0013,  0.0015,  0.0007,  0.0002],\n          [ 0.0015,  0.0009,  0.0010,  0.0017,  0.0008]]],\n\n\n        [[[-0.0014, -0.0009, -0.0016, -0.0024, -0.0025],\n          [-0.0012, -0.0012, -0.0020, -0.0025, -0.0031],\n          [-0.0006, -0.0004, -0.0007, -0.0012, -0.0020],\n          [-0.0003,  0.0004, -0.0002, -0.0010, -0.0020],\n          [-0.0003, -0.0004, -0.0018, -0.0026, -0.0035]]],\n\n\n        [[[ 0.0020,  0.0016, -0.0010, -0.0030, -0.0042],\n          [ 0.0010,  0.0001, -0.0018, -0.0023, -0.0028],\n          [ 0.0015,  0.0009, -0.0006, -0.0009, -0.0013],\n          [ 0.0023,  0.0013,  0.0004, -0.0002, -0.0027],\n          [ 0.0012, -0.0006, -0.0028, -0.0021, -0.0024]]],\n\n\n        [[[-0.0068, -0.0073, -0.0081, -0.0072, -0.0073],\n          [-0.0085, -0.0102, -0.0095, -0.0062, -0.0054],\n          [-0.0080, -0.0085, -0.0078, -0.0048, -0.0042],\n          [-0.0078, -0.0083, -0.0072, -0.0032, -0.0049],\n          [-0.0052, -0.0065, -0.0058, -0.0029, -0.0041]]],\n\n\n        [[[-0.0006, -0.0005, -0.0001, -0.0002,  0.0001],\n          [-0.0006, -0.0005, -0.0003,  0.0001,  0.0005],\n          [-0.0006, -0.0004,  0.0001,  0.0007,  0.0005],\n          [ 0.0001,  0.0005,  0.0008,  0.0010,  0.0011],\n          [ 0.0002,  0.0006,  0.0008,  0.0007,  0.0009]]]])\n"}],"source":"print(network.conv1.weight.grad)\nloss.backward()\nprint(network.conv1.weight.grad)\n"},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"previous prediction tensor(0.1000, dtype=torch.float64)\nprevious loss: 2.315969467163086\nnew loss: 2.240274429321289\nnew prediction: tensor(0.2000, dtype=torch.float64)\n"}],"source":"# Summary\nfrom torch import optim\n\n# train dataset download\ntrain_set= torchvision.datasets.FashionMNIST(\n    root= './data/FashionMNIST',\n    train= True,\n    download= True,\n    transform= transforms.Compose([\n        transforms.ToTensor()\n    ])\n)\n\n# network instance\nnetwork= Network()\n\n# train dataset loader\ntrain_loader= torch.utils.data.DataLoader(train_set, batch_size= 10)\n\n# optimizer function\noptimizer= optim.Adam(network.parameters(), lr=0.01)\n\n# load the first batch and iterate there after\nbatch = next(iter(data_loader))\nimages, labels= batch\n\n# prediction before anything\npreds= network(images)\nprint('previous prediction', accuracy(preds, labels))\n\n# initialize the loss function\nloss= nn.functional.cross_entropy(preds, labels)\nprint('previous loss:', loss.item())\n\n# initialize backward propagation\nloss.backward() # calculate gradients\noptimizer.step() # update weights\n\n# prediction after updating weights\npreds= network(images)\nloss= nn.functional.cross_entropy(preds, labels)\nprint('new loss:', loss.item())\nprint('new prediction:', accuracy(preds, labels))\n"}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}